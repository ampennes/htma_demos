{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Anthony's ramblings on making (and documenting) things Purpose I think the best learning is done ~1 on 1 or in very small groups. This has lots of positive side effects such as fostering stronger intra section communities but has the drawback of pulling from an immutable bucket of time. No matter how hard you work there are only 168 hours in the week so serial processes that hinge on the efforts of a small number of individuals slow processes down which results in our case in an unfortunate necessity to limit enrollment in this course. In an effort to reduce that I've started a series of guides to help supplement my own time and answer many of the frequently asked questions throughout this course. My response time on gitlab/email/cell is pretty quick but the search bar here doesn't sleep. I still think people are better as they can tailor explanations to an individual's background, expand upon unclear statements, and understand facial expressions but for many issues I hope this site will suffice or at least serve as a bandaid until a better solution exists. Additionally I think that our documentation could always be better and I hope to make a number of physical examples that live in the lab and can be interacted with in order to help get the creative juices flowing for the folks like myself that struggle coming up with creative and useful weekly goals when they are given assignments that are so open-ended. I like playing with things in person and find that it leads to thoughts like \"man this is cool but what if it did this instead?\" which are great jumping off points for the week. Finally I'll probably throw machine guides here for easy reference (note these vary vastly from shop to shop) and this is a bit of an excuse to play with different processes and make fun things while forcing documentation that I usually eschew. Philosophy While I have you on the main page I want to take the time to give a little bit of my personal advice for this class as well as life, the universe and everything. The biggest single bit of advice that I can possible give is to ask for help. I don't bite, no questions are stupid, and nothing makes me more upset than hearing after the fact about students who struggled alone for hours unable to solve an issue through the dead of night and just bashed their head against the wall for hours. There is a constructive amount of struggle in life but this is certainly not it. So much of problem solving hinges on prior knowledge that is often taken for granted and it is our job to help supplement your knowledge and intuition. Sometimes 90% of the problem is simply knowing what word to google or hinges on some misknomer such as thinking that mil is short for millimeter. Anyway please just speak up, I promise the social awkwardness will decrease over time and it'll get easier. I'm accessible via email, the issue tracker (often preferred so the same question serves as learning for others who undoubetly have it too), and my personal number is on our internal docs I just try not to post it on this public facing webpage. Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. mkdocs gh-build - from within the web folder to push new chages to ampennes.github.io/htma-demos Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-anthonys-ramblings-on-making-and-documenting-things","text":"","title":"Welcome to Anthony's ramblings on making (and documenting) things"},{"location":"#purpose","text":"I think the best learning is done ~1 on 1 or in very small groups. This has lots of positive side effects such as fostering stronger intra section communities but has the drawback of pulling from an immutable bucket of time. No matter how hard you work there are only 168 hours in the week so serial processes that hinge on the efforts of a small number of individuals slow processes down which results in our case in an unfortunate necessity to limit enrollment in this course. In an effort to reduce that I've started a series of guides to help supplement my own time and answer many of the frequently asked questions throughout this course. My response time on gitlab/email/cell is pretty quick but the search bar here doesn't sleep. I still think people are better as they can tailor explanations to an individual's background, expand upon unclear statements, and understand facial expressions but for many issues I hope this site will suffice or at least serve as a bandaid until a better solution exists. Additionally I think that our documentation could always be better and I hope to make a number of physical examples that live in the lab and can be interacted with in order to help get the creative juices flowing for the folks like myself that struggle coming up with creative and useful weekly goals when they are given assignments that are so open-ended. I like playing with things in person and find that it leads to thoughts like \"man this is cool but what if it did this instead?\" which are great jumping off points for the week. Finally I'll probably throw machine guides here for easy reference (note these vary vastly from shop to shop) and this is a bit of an excuse to play with different processes and make fun things while forcing documentation that I usually eschew.","title":"Purpose"},{"location":"#philosophy","text":"While I have you on the main page I want to take the time to give a little bit of my personal advice for this class as well as life, the universe and everything. The biggest single bit of advice that I can possible give is to ask for help. I don't bite, no questions are stupid, and nothing makes me more upset than hearing after the fact about students who struggled alone for hours unable to solve an issue through the dead of night and just bashed their head against the wall for hours. There is a constructive amount of struggle in life but this is certainly not it. So much of problem solving hinges on prior knowledge that is often taken for granted and it is our job to help supplement your knowledge and intuition. Sometimes 90% of the problem is simply knowing what word to google or hinges on some misknomer such as thinking that mil is short for millimeter. Anyway please just speak up, I promise the social awkwardness will decrease over time and it'll get easier. I'm accessible via email, the issue tracker (often preferred so the same question serves as learning for others who undoubetly have it too), and my personal number is on our internal docs I just try not to post it on this public facing webpage.","title":"Philosophy"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. mkdocs gh-build - from within the web folder to push new chages to ampennes.github.io/htma-demos","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"3Dprinting/","text":"What is worth covering here? - parts within parts - hinges (discreet and living) - multi-material - a fuzzy skin example - print time comparison? - crush test with varying infill and num walls? - A bit on stress and strain - some quick FEA results for different living hinge designs?","title":"3D Printing"},{"location":"imageCompression/","text":"Image compression and why it matters Why bother? Storage is cheap, data is fast, sites have low traffic so why do we even care about this? Well there are a couple reasons why I think it is still valid to expect people to have some discipline here. I won't bore you with my tales of growing up with dialup or no internet at all but that really wasn't very long ago. I think the story here is multi-faceted. First it is a good intro to the concept of resolution which is really vital when working with microcontrollers. Say you want to do a fancy computer vision project but you're running on a rather limited microcontroller. If you spend money on a super awesome camera in order to resolve the smallest feature size possible you'll find that your vision algorithms are going to run at rates far lower than is acceptable because you can't handle that amount of information at once so too much data can in fact be a problem in some cases. To get back to concrete examples here this directly impacts the load time of your page and with uncompressed images can become a bit of a drag, and given the nature of how our repos are setup you're all maintaining a local copy of everything that each student in your section commits. For those of you trying to stretch out that 256GB harddrive to do everything you need it to there is a big difference between a .raw image shot from a fancy iphone taking up nearly 100MB is undeniably beautiful but if you do that frequently that has a very measurable impact on your friends. Additionally git maintains a complete version history so getting rid of things completely is intentionally difficult which means it is much easier to do things correctly from the start. Finally the default image size is often much larger than the screen of the device you are viewing it with so a resize will make it fit into your page a little bit better. Finally we should recognize that small deltas across O(100) students make big differences. You're probably familiar with the concept of compound interest (if not tldr you have have an online bank with a high interest saving account) or have seen the motivational poster with 1.01^365=37.8. Here the analog is that the final size of our combined repos can vary drastically based on what folks do over the semester. How? I run windows machines so that is what this tutorial will focus around. The same should work on other OS with the exception of the .bat file but at this point MacOS has built in a pretty useful file compression tool directly in the menu so this is perhaps less necessary for you unless you want to batch process them. First download ImageMagick from here To match some of Neil's tutorials make sure to click \"install legacy utilities\" as you go through the install process. Make a note of the install location as we will likely want to add it to our path later. Now there are a couple ways to convert and compress your files. First it is important to understand what we are trying to do and why. The tldr is PNG is a pretty format that is kinda bloated and far higher quality than you really need on a webpage. In order to be nice to both our server and the visitors to your site we'll convert it to a jpeg and do a little compression which will speed up load times, reduce file sizes, and not really impact your file quality. Neil is a stickler for this as small deltas times ~100 students have big repurcussions. If you installed the legacy commands his scripts here are super helpful If those fail it is probably because you forgot to hit that checkbox while installing but don't worry as there is a perhaps even cooler way to handle this upcoming. First the new commands for those of you that want individual control: magick input.png output.jpg Would convert a given file from png to jpg. sticking a resize flag in there will change the physical size of the image (many of your phones take images that default to far larger than your laptop screen) Resize can take either percentages or XX*YY pixel sizes magick input.png -resize 50% output.jpg finally the quality tag is used to specify the amount of compression to apply to your image. A full command might look like: magick input.png -resize 50% -quality 50% output.jpg In my quick testing this brought an image from 500KB to 22KB without much issue. to verify this works we can inspect the sizes of all the files in a directory by using ls -la if you're in a unix friendly command shell. The output is in bytes. Now it is important that we don't throw away our efforts by accidentally committing both the compressed and uncompressed files in our repo. Something like git add * is dangerous here because it will do exactly what you tell it to which is add everything. There are many workarounds, only add specific files, move uncompressed images to another directory, update your git ignore, I'll leave that up to the discretion of the reader. My 2 cents is to not delete the original image in case you learn that something in the process broke, you compressed a bit too much, or some other issue popped up. You may however decide that I am a digital hoarder and you see no need for the original file anymore. To each their own. Now with a little extra magic (no k) and .bat files we can make a quick script that runs through a folder, converts and compresses every png, and is callable from anywhere in our system. For some this may sound daunting but let me remind you that I am technically a mechanical engieer and I figured this out so you certainly can too! So .bat files are just scripts that we can bundle and call and if we add them to our default path we can do this from anywhere in our system. A little extra trick is that you can actually call a script right from your file explorer without even opening a terminal! Kinda nifty. For me the script looks like: @ECHO OFF FOR %%a in (*.png) DO magick %%a -quality 50%% -resize 50%% %%~na_small.jpg and I saved this as compressAll.bat because I thought the name was appropriate. You could make some tweaks here if you say normally saved images in another format or wanted to play with compression rates ec. This is a template for you to have fun with as well as a useful tool. In a perfect world I kinda want 2 or 3 of these scripts. One to handle images from my phone which may be another format and likely want more compression, one to handle screenshots that I took locally which need less compression, and a third for outliers. It is a little annoying with the escaping % characters but what this does is parse the folder, find all png files, resize, compress, and convert them to jpg files and append _small to their file names. The file name suffix is perhaps not mandatory if your're changing image extensions but I find it a useful convention. Now let's make this script callable from anywhere by adding it to your path which is essentially the list of places your computer will look if you try to call a file or command that isn't located where your terminal currently is. You can kinda think of this as the difference between local and global variables in most languages. First lets make a permanent home for this script that is easy to access and has room to make more changes in the future. I just made a folder in Documents called Scripts. Short, simple, easy to remember. The path to this folder is then C:\\Users\\Anthony\\Documents\\Scripts. Some of y'all use things like onedrive or have other kinda funky installs so an easy way to see the file path in the file explorer is to just click in the bar at the top and copy it from there. Make sure to copy compressAll.bat into this folder. Now we need to update your path to include this location. The route to get here is a little convoluted but it isn't too bad. First open the windows search bar and type path. The top option should be \"edit the system enviornment variables\" Open that and click the \"enviornment variables\" button in the bottom right. The second element in the top list should be PATH. Click that and then click edit Now a new window will open. Hit new and paste the folder path into the cell then save and close everything. Now if you have any terminal instances open you should close them in order for the changes to take effect. Bonus Finally a little bonus on using .gitignore: Sometimes I like being lazy. I don't want to deal with remembering what not to git add and just use that nice wildcard to get everything. That would be a bit of an issue here because my uncompressed images still exist and it would defeat the whole point of this endeavor to compress and convert them if they ended up in the repo anyway. To fix this we'll leverage the .gitignore file. This is really designed to not track files with your credentials or API keys etc (which are embarassingly easy to find across public git repos these days) tldr to use this go to the top level of your repo and make a file called .gitignore Inside that file I am simply going to put *.png and then save it. That way whenever you add anything in git it will overlook anything that ends in .png entirely and save you from accidentally tracking a file that you shouldn't.","title":"ImageCompression"},{"location":"imageCompression/#image-compression-and-why-it-matters","text":"","title":"Image compression and why it matters"},{"location":"imageCompression/#why-bother","text":"Storage is cheap, data is fast, sites have low traffic so why do we even care about this? Well there are a couple reasons why I think it is still valid to expect people to have some discipline here. I won't bore you with my tales of growing up with dialup or no internet at all but that really wasn't very long ago. I think the story here is multi-faceted. First it is a good intro to the concept of resolution which is really vital when working with microcontrollers. Say you want to do a fancy computer vision project but you're running on a rather limited microcontroller. If you spend money on a super awesome camera in order to resolve the smallest feature size possible you'll find that your vision algorithms are going to run at rates far lower than is acceptable because you can't handle that amount of information at once so too much data can in fact be a problem in some cases. To get back to concrete examples here this directly impacts the load time of your page and with uncompressed images can become a bit of a drag, and given the nature of how our repos are setup you're all maintaining a local copy of everything that each student in your section commits. For those of you trying to stretch out that 256GB harddrive to do everything you need it to there is a big difference between a .raw image shot from a fancy iphone taking up nearly 100MB is undeniably beautiful but if you do that frequently that has a very measurable impact on your friends. Additionally git maintains a complete version history so getting rid of things completely is intentionally difficult which means it is much easier to do things correctly from the start. Finally the default image size is often much larger than the screen of the device you are viewing it with so a resize will make it fit into your page a little bit better. Finally we should recognize that small deltas across O(100) students make big differences. You're probably familiar with the concept of compound interest (if not tldr you have have an online bank with a high interest saving account) or have seen the motivational poster with 1.01^365=37.8. Here the analog is that the final size of our combined repos can vary drastically based on what folks do over the semester.","title":"Why bother?"},{"location":"imageCompression/#how","text":"I run windows machines so that is what this tutorial will focus around. The same should work on other OS with the exception of the .bat file but at this point MacOS has built in a pretty useful file compression tool directly in the menu so this is perhaps less necessary for you unless you want to batch process them. First download ImageMagick from here To match some of Neil's tutorials make sure to click \"install legacy utilities\" as you go through the install process. Make a note of the install location as we will likely want to add it to our path later. Now there are a couple ways to convert and compress your files. First it is important to understand what we are trying to do and why. The tldr is PNG is a pretty format that is kinda bloated and far higher quality than you really need on a webpage. In order to be nice to both our server and the visitors to your site we'll convert it to a jpeg and do a little compression which will speed up load times, reduce file sizes, and not really impact your file quality. Neil is a stickler for this as small deltas times ~100 students have big repurcussions. If you installed the legacy commands his scripts here are super helpful If those fail it is probably because you forgot to hit that checkbox while installing but don't worry as there is a perhaps even cooler way to handle this upcoming. First the new commands for those of you that want individual control: magick input.png output.jpg Would convert a given file from png to jpg. sticking a resize flag in there will change the physical size of the image (many of your phones take images that default to far larger than your laptop screen) Resize can take either percentages or XX*YY pixel sizes magick input.png -resize 50% output.jpg finally the quality tag is used to specify the amount of compression to apply to your image. A full command might look like: magick input.png -resize 50% -quality 50% output.jpg In my quick testing this brought an image from 500KB to 22KB without much issue. to verify this works we can inspect the sizes of all the files in a directory by using ls -la if you're in a unix friendly command shell. The output is in bytes. Now it is important that we don't throw away our efforts by accidentally committing both the compressed and uncompressed files in our repo. Something like git add * is dangerous here because it will do exactly what you tell it to which is add everything. There are many workarounds, only add specific files, move uncompressed images to another directory, update your git ignore, I'll leave that up to the discretion of the reader. My 2 cents is to not delete the original image in case you learn that something in the process broke, you compressed a bit too much, or some other issue popped up. You may however decide that I am a digital hoarder and you see no need for the original file anymore. To each their own. Now with a little extra magic (no k) and .bat files we can make a quick script that runs through a folder, converts and compresses every png, and is callable from anywhere in our system. For some this may sound daunting but let me remind you that I am technically a mechanical engieer and I figured this out so you certainly can too! So .bat files are just scripts that we can bundle and call and if we add them to our default path we can do this from anywhere in our system. A little extra trick is that you can actually call a script right from your file explorer without even opening a terminal! Kinda nifty. For me the script looks like: @ECHO OFF FOR %%a in (*.png) DO magick %%a -quality 50%% -resize 50%% %%~na_small.jpg and I saved this as compressAll.bat because I thought the name was appropriate. You could make some tweaks here if you say normally saved images in another format or wanted to play with compression rates ec. This is a template for you to have fun with as well as a useful tool. In a perfect world I kinda want 2 or 3 of these scripts. One to handle images from my phone which may be another format and likely want more compression, one to handle screenshots that I took locally which need less compression, and a third for outliers. It is a little annoying with the escaping % characters but what this does is parse the folder, find all png files, resize, compress, and convert them to jpg files and append _small to their file names. The file name suffix is perhaps not mandatory if your're changing image extensions but I find it a useful convention. Now let's make this script callable from anywhere by adding it to your path which is essentially the list of places your computer will look if you try to call a file or command that isn't located where your terminal currently is. You can kinda think of this as the difference between local and global variables in most languages. First lets make a permanent home for this script that is easy to access and has room to make more changes in the future. I just made a folder in Documents called Scripts. Short, simple, easy to remember. The path to this folder is then C:\\Users\\Anthony\\Documents\\Scripts. Some of y'all use things like onedrive or have other kinda funky installs so an easy way to see the file path in the file explorer is to just click in the bar at the top and copy it from there. Make sure to copy compressAll.bat into this folder. Now we need to update your path to include this location. The route to get here is a little convoluted but it isn't too bad. First open the windows search bar and type path. The top option should be \"edit the system enviornment variables\" Open that and click the \"enviornment variables\" button in the bottom right. The second element in the top list should be PATH. Click that and then click edit Now a new window will open. Hit new and paste the folder path into the cell then save and close everything. Now if you have any terminal instances open you should close them in order for the changes to take effect.","title":"How?"},{"location":"imageCompression/#bonus","text":"Finally a little bonus on using .gitignore: Sometimes I like being lazy. I don't want to deal with remembering what not to git add and just use that nice wildcard to get everything. That would be a bit of an issue here because my uncompressed images still exist and it would defeat the whole point of this endeavor to compress and convert them if they ended up in the repo anyway. To fix this we'll leverage the .gitignore file. This is really designed to not track files with your credentials or API keys etc (which are embarassingly easy to find across public git repos these days) tldr to use this go to the top level of your repo and make a file called .gitignore Inside that file I am simply going to put *.png and then save it. That way whenever you add anything in git it will overlook anything that ends in .png entirely and save you from accidentally tracking a file that you shouldn't.","title":"Bonus"},{"location":"lasers/","text":"Laser cutting Safety Fire is wonderful and we wouldn't be where we are today had we not learned to harness it long ago however fire in our shops is something that we should do our best to minimize. Aditionally there are lots of harmful vapors that can be released as combustion byproducts sowe need to be rather strict with what materials are allowed to be cut in our spaces and what ones should never enter the cutter. When in doubt always chat with your shop staff. Parameter search Now asking carboard to not light on fight as you blast it with enough energy to vaporize it sounds like a little bit of an oxymoron but there is actually a relatively wide parameter space in which you get good cuts without risking sending your machine up in smoke. We (or at least I) can't take a mathematical approach to solving for a range of parameters that will cut your material so we must take an empirical one. Let's start my making sure we live in the \"too fast to cut through but won't burn\" regime. One assumprion that we will make to simplify the process is that our speed should be locked at 100% in order to finish the job as quickly as possible. This will increase our throughput which is very impportant for serial jobs in shops with few machines, and it turns our 2D parameter search into an easier 1D search. This is an assumption that will hold true for cardboard and even most acrylic on our lasers but it does start to fail on thicker, denser materials that take more time to cut. Say you only cared about 5% tolerance then optimizing a 2D search would require ~400 tests and comparisons to ~20 for a 1D search so even an imperfect assumption saves us a lot of effort. In other words we want to simplify this to this Now I won't give you numbers because that defeats the point of the exercise but we probably want to start around say 2% power and 100% speed and increase your power a couple percent at a time. This will feel a little disappointing at first as you won't cut through all the way but it won't be a ball of fire... Don't forget to make note of the nice engraving parameter space that you'll likely pass through as you iterate to cutting. It sounds funky but you can in fact engrave some pretty decent detail into cardboard. Cardboard Do remember that while kinda nice this is still cardboard. Load bearing structures are certainly possible but they take quite a bit of thought and finesse. Joint types might be a little more limiting but nice pressfits are possible. It is also quite anisotropic with different stiffnessess based on your loading direction and the orientation of the corrugations between the layers. Feature size There isn't a hard rule here but do remember the corrugated nature of cardboard. If you decrease your feature size beneath the corrugation pitch there is no guarantee that your top and bottom layers are actually connected together. Additionally very small features become a fire hazard so you might need to adjust your speed/power settings in order to safely cut intricate details. Kerf Kerf is an important feature in many subtractive machining operations which boils down to the width of material removed by your tool. In this case our tool is a beam of light so it is small but it isn't infinitesimal and if you don't account for it then your parts won't fit together as well as they should. The easiest way to measure the kerf is to cut an object of a known size and measure what the actual output is. Say I wanted a 1cm square and got a .95cm square then I would know my kerf is 5mm and if I offset my perimeters by +/- 2.5mm then my parts should come out the exact size that I want them. Why isn't this built in to the machine in 2024 given that the process is so simple? I don't know. There is some variance between whether you are keeping the positive or the negative and kerf can be a function of material thickness but getting at least close is pretty easy. I bet it would be a fun internship at Epilog though. Parametric What does this mean and why is it important again? Say hypothetically you need to make a box. You spend a while modelling everything in your cad package of choice and after a couple hours you have the world's best box. Once you're done you get a call asking if you could change the dimensions of that box a bit to add room for some other component that was forgotten about or you find that your joints were a little loose and you wanted to tighten them all up. In the best case scenario you have to figure out which sketches set the relevant parameters, find them in your timeline and make the edits then cross your fingers that everything rebuilds properly and you don't end up with missing references, tabs/slots that don't line up, and a pile of spaghetti the untangling of which can take as much time as starting from scratch. Well most cad tools these days arealso somewhat decent programming environments. When inputting a dimension instead of doing 1 cm you can do something like overall_width = 1 in This might feel a bit clunky at first but I assure you it becomes very powerful. You can also do lots of math in the same panel. Something like (overall_width/num_connections)+kerf-tolerance is perfectly valid as long as you have all of these terms defined. Done properly lots of relative dimensions will let you make very flexible models that can build and rebuild in vastly different sizes and configurations without any issues. In order to access the list of these parameters from the whole model at once you can use the modify->change parameters button. Working this way has a little bit of a learning curve but the benefit far outweighs the input effort for all but the simplest of models. Fits How things fit together is a very well studied field with ample resources available to make sure that stationary things stay stationary and sliding things slide. This however tends to be done in more homogenous materials that are a little more predictable than cardboard but starting from numbers that work in other materials is certainly better than guessing. I usually use bolt hole charts as a rule of thumb and then adjust as needed if I find my fits are a little loose. This really should be done on the small scale as a group project which will then feed forward into your designs. A bit of effort up front will save you from hours of cutting out big complex parts to realize that they're all trash and you need to start again. I try not to meddle in the process of an individual but some light steering here to avoid big sinks of wasted effort is an exception that I'll happily make. Joints Bendy bits Perhaps the most fun part of this assignment is thinking through and making exceptionally flexible parts starting from a rigid material. This is another well studied field (google flexure design) but given the low stakes here you can abandon the mathematical approach and take a more artistic approach to engineering if you so choose. The guiding factors somewhat simplify down to minimizing strain which is defined as the change in length of a beam over its original length. essentially if we can spread the same bend over long windy pieces rather than short stubby ones we will get a more flexible material and fewer kinked sheets. The specific choice of pattern and the dimensions of it are important factors in determining how a piece is going to bend that I'll come back with more detail on in a future iteraton of this page. Actually using the machine Machine trainings should always be done in person but here I'll list some important bits that either tend to be forgotten or are simply worth saying twice. Going too fast There are 2 main cases where you won't want to run your laser cutter at 100% speed. The first is simply once you've maxed the speed and the power and your material is still not being cut in one pass. Then you simply reduce the speed in order to spend more time dumping energy into the same spot on the material hopefully burning/ablating it away and managing to make it all the way through your material in one go. The next case however is more interesting. If you are doing very finely detailed work you might find that the control loop around your machine has some ringing in it which causes noticable deteoriations in performance. Below I've included some preliminary testing that I was doing on cutting circuits with a cheap fiber laser. Those are supposed to be straight line segments but you can see the obvious ringing as the machine shakes around. In this case reducing the speed could be necessary if you really want to push the boundaries of what is possible. Here it was taking multiple passes to cut through and the lack of proper tracking in the corners caused them to simply not heat the copper enough to cut through. Cutting twice Another edge case worth being cognizant of is simply sometimes you need to cut twice. If you haven't disturbed your material nothing is stopping you from just running the same job again if you determine that your first attempt didn't cut all the way through. These are imperfect machines cutting imperfect stock and sometimes you just gotta go again. Warped stock changes the where the focal plane sits in your material which changes the spot size and the energy density which could cause it to not complete the cut. Natural materials can have knotty or extra dense areas that change the physical properties, the laser cartridge can get a little toasty and derate its power to help compensate etc etc. Some inspiration I hate whitespace. I much prefer bounded problems and a bit of rails in order to help get the creative juices flowing. The real inspiration for this site was to begin documenting a series of demos that like-minded folks could play with before they get started on their assignment for the week to help seed cool ideas and the \"but what if...\" thought process. Everything up to this point has mostly been an afterthought that I figured I might as well document while on the journey to this destination as I thought it could come in handy. This is one of the weeks in which the need for this isn't particularly strong so perhaps I'll just use it as a deeper dive into flexible bits and pattern choices.","title":"Laser Cutting"},{"location":"lasers/#laser-cutting","text":"","title":"Laser cutting"},{"location":"lasers/#safety","text":"Fire is wonderful and we wouldn't be where we are today had we not learned to harness it long ago however fire in our shops is something that we should do our best to minimize. Aditionally there are lots of harmful vapors that can be released as combustion byproducts sowe need to be rather strict with what materials are allowed to be cut in our spaces and what ones should never enter the cutter. When in doubt always chat with your shop staff.","title":"Safety"},{"location":"lasers/#parameter-search","text":"Now asking carboard to not light on fight as you blast it with enough energy to vaporize it sounds like a little bit of an oxymoron but there is actually a relatively wide parameter space in which you get good cuts without risking sending your machine up in smoke. We (or at least I) can't take a mathematical approach to solving for a range of parameters that will cut your material so we must take an empirical one. Let's start my making sure we live in the \"too fast to cut through but won't burn\" regime. One assumprion that we will make to simplify the process is that our speed should be locked at 100% in order to finish the job as quickly as possible. This will increase our throughput which is very impportant for serial jobs in shops with few machines, and it turns our 2D parameter search into an easier 1D search. This is an assumption that will hold true for cardboard and even most acrylic on our lasers but it does start to fail on thicker, denser materials that take more time to cut. Say you only cared about 5% tolerance then optimizing a 2D search would require ~400 tests and comparisons to ~20 for a 1D search so even an imperfect assumption saves us a lot of effort. In other words we want to simplify this to this Now I won't give you numbers because that defeats the point of the exercise but we probably want to start around say 2% power and 100% speed and increase your power a couple percent at a time. This will feel a little disappointing at first as you won't cut through all the way but it won't be a ball of fire... Don't forget to make note of the nice engraving parameter space that you'll likely pass through as you iterate to cutting. It sounds funky but you can in fact engrave some pretty decent detail into cardboard.","title":"Parameter search"},{"location":"lasers/#cardboard","text":"Do remember that while kinda nice this is still cardboard. Load bearing structures are certainly possible but they take quite a bit of thought and finesse. Joint types might be a little more limiting but nice pressfits are possible. It is also quite anisotropic with different stiffnessess based on your loading direction and the orientation of the corrugations between the layers.","title":"Cardboard"},{"location":"lasers/#feature-size","text":"There isn't a hard rule here but do remember the corrugated nature of cardboard. If you decrease your feature size beneath the corrugation pitch there is no guarantee that your top and bottom layers are actually connected together. Additionally very small features become a fire hazard so you might need to adjust your speed/power settings in order to safely cut intricate details.","title":"Feature size"},{"location":"lasers/#kerf","text":"Kerf is an important feature in many subtractive machining operations which boils down to the width of material removed by your tool. In this case our tool is a beam of light so it is small but it isn't infinitesimal and if you don't account for it then your parts won't fit together as well as they should. The easiest way to measure the kerf is to cut an object of a known size and measure what the actual output is. Say I wanted a 1cm square and got a .95cm square then I would know my kerf is 5mm and if I offset my perimeters by +/- 2.5mm then my parts should come out the exact size that I want them. Why isn't this built in to the machine in 2024 given that the process is so simple? I don't know. There is some variance between whether you are keeping the positive or the negative and kerf can be a function of material thickness but getting at least close is pretty easy. I bet it would be a fun internship at Epilog though.","title":"Kerf"},{"location":"lasers/#parametric","text":"What does this mean and why is it important again? Say hypothetically you need to make a box. You spend a while modelling everything in your cad package of choice and after a couple hours you have the world's best box. Once you're done you get a call asking if you could change the dimensions of that box a bit to add room for some other component that was forgotten about or you find that your joints were a little loose and you wanted to tighten them all up. In the best case scenario you have to figure out which sketches set the relevant parameters, find them in your timeline and make the edits then cross your fingers that everything rebuilds properly and you don't end up with missing references, tabs/slots that don't line up, and a pile of spaghetti the untangling of which can take as much time as starting from scratch. Well most cad tools these days arealso somewhat decent programming environments. When inputting a dimension instead of doing 1 cm you can do something like overall_width = 1 in This might feel a bit clunky at first but I assure you it becomes very powerful. You can also do lots of math in the same panel. Something like (overall_width/num_connections)+kerf-tolerance is perfectly valid as long as you have all of these terms defined. Done properly lots of relative dimensions will let you make very flexible models that can build and rebuild in vastly different sizes and configurations without any issues. In order to access the list of these parameters from the whole model at once you can use the modify->change parameters button. Working this way has a little bit of a learning curve but the benefit far outweighs the input effort for all but the simplest of models.","title":"Parametric"},{"location":"lasers/#fits","text":"How things fit together is a very well studied field with ample resources available to make sure that stationary things stay stationary and sliding things slide. This however tends to be done in more homogenous materials that are a little more predictable than cardboard but starting from numbers that work in other materials is certainly better than guessing. I usually use bolt hole charts as a rule of thumb and then adjust as needed if I find my fits are a little loose. This really should be done on the small scale as a group project which will then feed forward into your designs. A bit of effort up front will save you from hours of cutting out big complex parts to realize that they're all trash and you need to start again. I try not to meddle in the process of an individual but some light steering here to avoid big sinks of wasted effort is an exception that I'll happily make.","title":"Fits"},{"location":"lasers/#joints","text":"","title":"Joints"},{"location":"lasers/#bendy-bits","text":"Perhaps the most fun part of this assignment is thinking through and making exceptionally flexible parts starting from a rigid material. This is another well studied field (google flexure design) but given the low stakes here you can abandon the mathematical approach and take a more artistic approach to engineering if you so choose. The guiding factors somewhat simplify down to minimizing strain which is defined as the change in length of a beam over its original length. essentially if we can spread the same bend over long windy pieces rather than short stubby ones we will get a more flexible material and fewer kinked sheets. The specific choice of pattern and the dimensions of it are important factors in determining how a piece is going to bend that I'll come back with more detail on in a future iteraton of this page.","title":"Bendy bits"},{"location":"lasers/#actually-using-the-machine","text":"Machine trainings should always be done in person but here I'll list some important bits that either tend to be forgotten or are simply worth saying twice.","title":"Actually using the machine"},{"location":"lasers/#going-too-fast","text":"There are 2 main cases where you won't want to run your laser cutter at 100% speed. The first is simply once you've maxed the speed and the power and your material is still not being cut in one pass. Then you simply reduce the speed in order to spend more time dumping energy into the same spot on the material hopefully burning/ablating it away and managing to make it all the way through your material in one go. The next case however is more interesting. If you are doing very finely detailed work you might find that the control loop around your machine has some ringing in it which causes noticable deteoriations in performance. Below I've included some preliminary testing that I was doing on cutting circuits with a cheap fiber laser. Those are supposed to be straight line segments but you can see the obvious ringing as the machine shakes around. In this case reducing the speed could be necessary if you really want to push the boundaries of what is possible. Here it was taking multiple passes to cut through and the lack of proper tracking in the corners caused them to simply not heat the copper enough to cut through.","title":"Going too fast"},{"location":"lasers/#cutting-twice","text":"Another edge case worth being cognizant of is simply sometimes you need to cut twice. If you haven't disturbed your material nothing is stopping you from just running the same job again if you determine that your first attempt didn't cut all the way through. These are imperfect machines cutting imperfect stock and sometimes you just gotta go again. Warped stock changes the where the focal plane sits in your material which changes the spot size and the energy density which could cause it to not complete the cut. Natural materials can have knotty or extra dense areas that change the physical properties, the laser cartridge can get a little toasty and derate its power to help compensate etc etc.","title":"Cutting twice"},{"location":"lasers/#some-inspiration","text":"I hate whitespace. I much prefer bounded problems and a bit of rails in order to help get the creative juices flowing. The real inspiration for this site was to begin documenting a series of demos that like-minded folks could play with before they get started on their assignment for the week to help seed cool ideas and the \"but what if...\" thought process. Everything up to this point has mostly been an afterthought that I figured I might as well document while on the journey to this destination as I thought it could come in handy. This is one of the weeks in which the need for this isn't particularly strong so perhaps I'll just use it as a deeper dive into flexible bits and pattern choices.","title":"Some inspiration"},{"location":"parametric/","text":"Parametric design CAD packages Design ethos","title":"Parametric Cad"},{"location":"parametric/#parametric-design","text":"","title":"Parametric design"},{"location":"parametric/#cad-packages","text":"","title":"CAD packages"},{"location":"parametric/#design-ethos","text":"","title":"Design ethos"},{"location":"programming/","text":"Embedded Programming Disclaimer I should point out that while an EECS employee I technically was a MechE here and as such I am mostly a hodge podge of self taught lessons and certainly not the best individual for you to learn from so please take the below with a grain of salt. I strive to write mostly understandable code that trades optimization for readability. I believe execution time can be important but it is actually 2 execution speeds that I value. First the traditionl time taken to run your code but also the time taken to write your code. When you're just starting out code that takes 10x longer to run but uses a more understandable approach is sometimes totally fine. We're privileged enough at this point to be running on microcontrollers with large flash storage and blazing fast clocks so the lack of optimization might not have any measurable repurcussions in your project. Choosing a language This is a rather contentious topic but I'll try to summarize the main options. The choice is entirely up to you and I won't discourage creativety but we have 2 main options: some flavor of C (likely Arduino) and Micropython. The former has been around for a very long time, has a simply massive amount of library support, and a wealth of tutorials. The latter is relatively new and improving quickly but it can be a little harder to find relevant help for. That being said some folks simply find micropython easier and would prefer to work there. I think my advice for software choices tends to boil down to \"find which option works closer to how your brain does and stick with it\" I'm a bit more adept in debugging and helping solve issues in C but most of the issues are really like logic puzzles and with the proper debugging tools are somewhat language agnostic in the solve process. I think my biggest goal for this semester is to help folks learn how to break problems down and solve them methodologically rather than simply guessing until you land on the broken bit. Arduino setup I'm going to mostly rely on the Seeed documentation here as it is pretty well written and correct at this point. If you want to skip the dev boards and work directly with the chips (my preferred method) Jake has great documentation for that here Micropython setup Debugging strategies","title":"Embedded Programming"},{"location":"programming/#embedded-programming","text":"","title":"Embedded Programming"},{"location":"programming/#disclaimer","text":"I should point out that while an EECS employee I technically was a MechE here and as such I am mostly a hodge podge of self taught lessons and certainly not the best individual for you to learn from so please take the below with a grain of salt. I strive to write mostly understandable code that trades optimization for readability. I believe execution time can be important but it is actually 2 execution speeds that I value. First the traditionl time taken to run your code but also the time taken to write your code. When you're just starting out code that takes 10x longer to run but uses a more understandable approach is sometimes totally fine. We're privileged enough at this point to be running on microcontrollers with large flash storage and blazing fast clocks so the lack of optimization might not have any measurable repurcussions in your project.","title":"Disclaimer"},{"location":"programming/#choosing-a-language","text":"This is a rather contentious topic but I'll try to summarize the main options. The choice is entirely up to you and I won't discourage creativety but we have 2 main options: some flavor of C (likely Arduino) and Micropython. The former has been around for a very long time, has a simply massive amount of library support, and a wealth of tutorials. The latter is relatively new and improving quickly but it can be a little harder to find relevant help for. That being said some folks simply find micropython easier and would prefer to work there. I think my advice for software choices tends to boil down to \"find which option works closer to how your brain does and stick with it\" I'm a bit more adept in debugging and helping solve issues in C but most of the issues are really like logic puzzles and with the proper debugging tools are somewhat language agnostic in the solve process. I think my biggest goal for this semester is to help folks learn how to break problems down and solve them methodologically rather than simply guessing until you land on the broken bit.","title":"Choosing a language"},{"location":"programming/#arduino-setup","text":"I'm going to mostly rely on the Seeed documentation here as it is pretty well written and correct at this point. If you want to skip the dev boards and work directly with the chips (my preferred method) Jake has great documentation for that here","title":"Arduino setup"},{"location":"programming/#micropython-setup","text":"","title":"Micropython setup"},{"location":"programming/#debugging-strategies","text":"","title":"Debugging strategies"}]}